ord <- order(word_freq)
# Visually display the most frequent words
wordcloud(names(word_freq), word_freq, colors = brewer.pal(8, "Dark2"))
# Organize Histogram Data
# Create a Histogram by frequencies
WordFreq <- data.frame(word=names(word_freq), freq=word_freq)
p <- ggplot(subset(WordFreq, freq>20), aes(word, freq));
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
# perform content analysis to rank (hierarchical clustering)
distance_matrix <- dist(as.matrix(dtm), method = "euclidean")
hc <- hclust(as.dist(distance_matrix), method = "ward.D2")
plot(hc)
########### Text summarization
library(textmineR)
library(igraph)
embedding <- function(corpora, Number.of.Sentences, Number.of.Topics){
tcm <- CreateTcm(doc_vec = corpora$content,skipgram_window = 10, verbose=FALSE,cpus=2)
set.seed(123)
embeddings <- FitLdaModel(dtm=tcm,
k = Number.of.Topics,
iterations=200,
burnin=180,
alpha=0.1,
beta=0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus =2)
doc <- corpora$content
names(doc) <- corpora$id
gamma = embeddings$gamma
sums = summarizer(doc,gamma,Number.of.Sentences)
return(sums)
}
summarizer <- function(doc,gamma,Number.of.Sentences){
if(length(doc)>1){
return(sapply(doc,function(d) try(summarizer(d,gamma,Number.of.Sentences))))
}
sentences <- stringi::stri_split_boundaries(doc,type = 'sentence')[[ 1 ]]
names(sentences) <- seq_along(sentences)
dtm <- CreateDtm(sentences,ngram_window = c(1,1),verbose = FALSE,cpus = 2)
dtm <- dtm[rowSums(dtm) > 2, ]
vocab <- intersect(colnames(dtm), colnames(gamma))
dtm  <-  dtm / rowSums(dtm)
dtm_topic  <-  dtm[ ,vocab] %*% t(gamma[ , vocab ])
dtm_topic  <-  as.matrix(dtm_topic)
e_dist  <-  CalcHellingerDist(dtm_topic)
e_measure  <-  (1 - e_dist)*100
diag(e_measure)  <-  0
Closest.Neighbors = 5
e_measure1 <- apply(e_measure, 1, function(x){
x[x < sort(x, decreasing = TRUE)[Closest.Neighbors]] <- 0
x
})
e_measure2 <- pmax(e_measure1, t(e_measure1))
e_measure3 <- graph.adjacency(e_measure2, mode = "undirected", weighted = TRUE)
ev <- evcent(e_measure3)
result <- sentences[names(ev$vector)[order(ev$vector,decreasing = TRUE)[1:Number.of.Sentences]]]
result <- result[order(as.numeric(names(result)))]
result <- paste(result, collapse = " ")
return(result)
}
corpus_text_vec <- sapply(corpus.txt, as.character)
# Test perplexity and find the best number of topics.
library(topicmodels)
dtm_list <- lapply(corpus.txt, function(doc) {
# Preprocess the document
# Create a DTM for the document
dtm <- DocumentTermMatrix(doc)
return(dtm)
})
# create two empty vector
best_topic_num= c()
perplexity_values=c()
# Set the number of topics
num_topics <- 2:8
for (i in 1:10){
dtm_matrix <- as.matrix(dtm_list[[i]])
# remove the row with all zero
dtm_matrix <- dtm_matrix[rowSums(dtm_matrix) > 0, ]
# Fit the LDA model
for (j in num_topics){
lda_model <- LDA(dtm_matrix, k = j)
# Calculate perplexity
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
perplexity_values <- c(perplexity_values, perplexity)
}
# Find the index for the best perplexity values and record as topic number
best <- which.min(perplexity_values)+1
perplexity_values=c()
best_topic_num= c(best_topic_num,best)
}
# Set the number of topics
num_topics <- 2:8
for (i in 1:10){
dtm_matrix <- as.matrix(dtm_list[[i]])
# remove the row with all zero
dtm_matrix <- dtm_matrix[rowSums(dtm_matrix) > 0, ]
# Fit the LDA model
for (j in num_topics){
lda_model <- LDA(dtm_matrix, k = j)
# Calculate perplexity
perplexity <- perplexity(lda_model)
perplexity_values <- c(perplexity_values, perplexity)
}
# Find the index for the best perplexity values and record as topic number
best <- which.min(perplexity_values)+1
perplexity_values=c()
best_topic_num= c(best_topic_num,best)
}
?perplexity
dtm_matrix <- as.matrix(dtm_list[[1]])
lda_model <- LDA(dtm_matrix, k = 3)
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
dtm_list
dtm_matrix
perplexity <- perplexity(lda_model, dtm_matrix)
perplexity_values=c()
dtm_matrix <- as.matrix(dtm_list[[1]])
lda_model <- LDA(dtm_matrix, k = 3)
perplexity <- perplexity(lda_model, dtm_matrix)
num_topics <- 2:8
for (i in 1:10) {
dtm_matrix <- as(as.matrix(dtm_list[[i]]), "sparseMatrix")
# remove the row with all zero
dtm_matrix <- dtm_matrix[rowSums(dtm_matrix) > 0, ]
# Fit the LDA model
for (j in num_topics) {
lda_model <- LDA(dtm_matrix, k = j)
# Calculate perplexity
perplexity <- perplexity(lda_model)
perplexity_values <- c(perplexity_values, perplexity)
}
# Find the index for the best perplexity values and record as topic number
best <- which.min(perplexity_values) + 1
perplexity_values = c()
best_topic_num = c(best_topic_num, best)
}
best_topic_num
# create two empty vector
best_topic_num= c()
perplexity_values=c()
dtm_matrix <- as.matrix(dtm_list[[1]])
lda_model <- LDA(dtm_matrix, k = 3)
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
dtm_matrix
newdata=dtm_matrix
lda_model <- LDA(dtm_matrix, k = 3)
perplexity <- perplexity(lda_model, newdata=dtm_matrix)
lda_model
# create the dtm
dtm <- DocumentTermMatrix(corpus.txt)
dtm_list <- lapply(corpus.txt, function(doc) {
# Preprocess the document
# Create a DTM for the document
dtm <- DocumentTermMatrix(doc)
return(dtm)
})
# create two empty vector
best_topic_num= c()
perplexity_values=c()
dtm_matrix <- as.matrix(dtm_list[[1]])
lda_model <- LDA(dtm_matrix, k = 3)
perplexity <- perplexity(lda_model, newdata=dtm_matrix)
library(topicmodels)
library(Matrix)
# Test perplexity and find the best number of topics.
library(topicmodels)
dtm_list <- lapply(corpus.txt, function(doc) {
# Preprocess the document
# Create a DTM for the document
dtm <- DocumentTermMatrix(doc)
return(dtm)
})
# create two empty vector
best_topic_num= c()
perplexity_values=c()
dtm_matrix <- as.matrix(dtm_list[[1]])
lda_model <- LDA(dtm_matrix, k = 3)
perplexity <- perplexity(lda_model, newdata=dtm_matrix)
DocumentTermMatrix(corpus.txt)
# Create a DTM for the document
dtm <- DocumentTermMatrix(corpus.txt)
# create two empty vector
best_topic_num= c()
perplexity_values=c()
dtm_matrix <- as.matrix(dtm_list[[1]])
lda_model <- LDA(dtm_matrix, k = 3)
perplexity <- perplexity(lda_model, newdata=dtm_matrix)
# Set the number of topics
num_topics <- 2:8
for (i in 1:10){
dtm_matrix <- as.matrix(dtm_list[[i]])
# remove the row with all zero
dtm_matrix <- dtm_matrix[rowSums(dtm_matrix) > 0, ]
# Fit the LDA model
for (j in num_topics){
lda_model <- LDA(dtm_matrix, k = j)
# Calculate perplexity
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
perplexity_values <- c(perplexity_values, perplexity)
}
# Find the index for the best perplexity values and record as topic number
best <- which.min(perplexity_values)+1
perplexity_values=c()
best_topic_num= c(best_topic_num,best)
}
best_topic_num
dtm_matrix <- as.matrix(dtm)
# Set the number of topics
num_topics <- 5
# Fit the LDA model
lda_model <- LDA(dtm_matrix, k = num_topics)
# Calculate perplexity
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
print(perplexity)
dtm_matrix <- as(as.matrix(dtm_list[[1]]), "sparseMatrix")
dtm_matrix
# Set the number of topics
num_topics <- 5
# Fit the LDA model
lda_model <- LDA(dtm_matrix, k = num_topics)
# Calculate perplexity
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
library(tm)
library("readr")
library(NLP)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(proxy)
library(stringdist)
library(ggdendro)
library(SnowballC)
library(rvest)
library(stringr)
library(topicmodels)
library(Matrix)
# load all the txt file
files.txt <- list.files(path = getwd(),pattern = "\\.txt$")
# load them as Corpus
corpus.txt<- VCorpus(URISource(files.txt),
readerControl = list(reader = readPlain, language = "en"))
# Remove everything inside the bracket and bracket itself
remove_brackets <- function(x) {
gsub("\\([^()]*\\)", "", x)
}
corpus.txt <- tm_map(corpus.txt, content_transformer(remove_brackets))
# convert to lower
corpus.txt <- tm_map(corpus.txt, content_transformer(tolower))
# Remove stopwords
corpus.txt <- tm_map(corpus.txt, removeWords, stopwords("english"))
# Remove whitespace
corpus.txt <- tm_map(corpus.txt, stripWhitespace)
corpus.txt <- tm_map(corpus.txt, content_transformer(trimws))
# Remove empty lines
remove_empty_strings <- function(x) {
x <- unlist(strsplit(x, "\n"))
x <- x[x != ""]
}
corpus.txt <- tm_map(corpus.txt, content_transformer(remove_empty_strings))
corpus.txt[[6]]$content
# create the dtm
dtm <- DocumentTermMatrix(corpus.txt)
dtm <- removeSparseTerms(dtm,0.7)
# Term frequencies
word_freq <- colSums(as.matrix(dtm))
# Ordering the frequencies
ord <- order(word_freq)
# Visually display the most frequent words
wordcloud(names(word_freq), word_freq, colors = brewer.pal(8, "Dark2"))
# Organize Histogram Data
# Create a Histogram by frequencies
WordFreq <- data.frame(word=names(word_freq), freq=word_freq)
p <- ggplot(subset(WordFreq, freq>20), aes(word, freq));
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
# perform content analysis to rank (hierarchical clustering)
distance_matrix <- dist(as.matrix(dtm), method = "euclidean")
hc <- hclust(as.dist(distance_matrix), method = "ward.D2")
plot(hc)
########### Text summarization
library(textmineR)
library(igraph)
embedding <- function(corpora, Number.of.Sentences, Number.of.Topics){
tcm <- CreateTcm(doc_vec = corpora$content,skipgram_window = 10, verbose=FALSE,cpus=2)
set.seed(123)
embeddings <- FitLdaModel(dtm=tcm,
k = Number.of.Topics,
iterations=200,
burnin=180,
alpha=0.1,
beta=0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus =2)
doc <- corpora$content
names(doc) <- corpora$id
gamma = embeddings$gamma
sums = summarizer(doc,gamma,Number.of.Sentences)
return(sums)
}
summarizer <- function(doc,gamma,Number.of.Sentences){
if(length(doc)>1){
return(sapply(doc,function(d) try(summarizer(d,gamma,Number.of.Sentences))))
}
sentences <- stringi::stri_split_boundaries(doc,type = 'sentence')[[ 1 ]]
names(sentences) <- seq_along(sentences)
dtm <- CreateDtm(sentences,ngram_window = c(1,1),verbose = FALSE,cpus = 2)
dtm <- dtm[rowSums(dtm) > 2, ]
vocab <- intersect(colnames(dtm), colnames(gamma))
dtm  <-  dtm / rowSums(dtm)
dtm_topic  <-  dtm[ ,vocab] %*% t(gamma[ , vocab ])
dtm_topic  <-  as.matrix(dtm_topic)
e_dist  <-  CalcHellingerDist(dtm_topic)
e_measure  <-  (1 - e_dist)*100
diag(e_measure)  <-  0
Closest.Neighbors = 5
e_measure1 <- apply(e_measure, 1, function(x){
x[x < sort(x, decreasing = TRUE)[Closest.Neighbors]] <- 0
x
})
e_measure2 <- pmax(e_measure1, t(e_measure1))
e_measure3 <- graph.adjacency(e_measure2, mode = "undirected", weighted = TRUE)
ev <- evcent(e_measure3)
result <- sentences[names(ev$vector)[order(ev$vector,decreasing = TRUE)[1:Number.of.Sentences]]]
result <- result[order(as.numeric(names(result)))]
result <- paste(result, collapse = " ")
return(result)
}
corpus_text_vec <- sapply(corpus.txt, as.character)
# Test perplexity and find the best number of topics.
library(topicmodels)
dtm_list <- lapply(corpus.txt, function(doc) {
# Preprocess the document
# Create a DTM for the document
dtm <- DocumentTermMatrix(corpus.txt)
return(dtm)
})
# create two empty vector
best_topic_num= c()
perplexity_values=c()
dtm_matrix <- as.matrix(dtm)
dtm_matrix <- as(as.matrix(dtm_list[[1]]), "sparseMatrix")
# Set the number of topics
num_topics <- 5
# Fit the LDA model
lda_model <- LDA(dtm_matrix, k = num_topics)
# Calculate perplexity
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
dtm_matrix <- as.matrix(dtm)
# Set the number of topics
num_topics <- 5
# Fit the LDA model
lda_model <- LDA(dtm_matrix, k = num_topics)
# Calculate perplexity
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
print(perplexity)
library(tm)
library("readr")
library(NLP)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(proxy)
library(stringdist)
library(ggdendro)
library(SnowballC)
library(rvest)
library(stringr)
library(topicmodels)
library(Matrix)
# load all the txt file
files.txt <- list.files(path = getwd(),pattern = "\\.txt$")
# load them as Corpus
corpus.txt<- VCorpus(URISource(files.txt),
readerControl = list(reader = readPlain, language = "en"))
# Remove everything inside the bracket and bracket itself
remove_brackets <- function(x) {
gsub("\\([^()]*\\)", "", x)
}
corpus.txt <- tm_map(corpus.txt, content_transformer(remove_brackets))
# convert to lower
corpus.txt <- tm_map(corpus.txt, content_transformer(tolower))
# Remove stopwords
corpus.txt <- tm_map(corpus.txt, removeWords, stopwords("english"))
# Remove whitespace
corpus.txt <- tm_map(corpus.txt, stripWhitespace)
corpus.txt <- tm_map(corpus.txt, content_transformer(trimws))
# Remove empty lines
remove_empty_strings <- function(x) {
x <- unlist(strsplit(x, "\n"))
x <- x[x != ""]
}
corpus.txt <- tm_map(corpus.txt, content_transformer(remove_empty_strings))
corpus.txt[[6]]$content
# create the dtm
dtm <- DocumentTermMatrix(corpus.txt)
dtm <- removeSparseTerms(dtm,0.7)
# Term frequencies
word_freq <- colSums(as.matrix(dtm))
# Ordering the frequencies
ord <- order(word_freq)
# Visually display the most frequent words
wordcloud(names(word_freq), word_freq, colors = brewer.pal(8, "Dark2"))
# Organize Histogram Data
# Create a Histogram by frequencies
WordFreq <- data.frame(word=names(word_freq), freq=word_freq)
p <- ggplot(subset(WordFreq, freq>20), aes(word, freq));
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
# perform content analysis to rank (hierarchical clustering)
distance_matrix <- dist(as.matrix(dtm), method = "euclidean")
hc <- hclust(as.dist(distance_matrix), method = "ward.D2")
plot(hc)
########### Text summary
library(textmineR)
library(igraph)
embedding <- function(corpora, Number.of.Sentences, Number.of.Topics){
tcm <- CreateTcm(doc_vec = corpora$content,skipgram_window = 10, verbose=FALSE,cpus=2)
set.seed(123)
embeddings <- FitLdaModel(dtm=tcm,
k = Number.of.Topics,
iterations=200,
burnin=180,
alpha=0.1,
beta=0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus =2)
doc <- corpora$content
names(doc) <- corpora$id
gamma = embeddings$gamma
sums = summarizer(doc,gamma,Number.of.Sentences)
return(sums)
}
summarizer <- function(doc,gamma,Number.of.Sentences){
if(length(doc)>1){
return(sapply(doc,function(d) try(summarizer(d,gamma,Number.of.Sentences))))
}
sentences <- stringi::stri_split_boundaries(doc,type = 'sentence')[[ 1 ]]
names(sentences) <- seq_along(sentences)
dtm <- CreateDtm(sentences,ngram_window = c(1,1),verbose = FALSE,cpus = 2)
dtm <- dtm[rowSums(dtm) > 2, ]
vocab <- intersect(colnames(dtm), colnames(gamma))
dtm  <-  dtm / rowSums(dtm)
dtm_topic  <-  dtm[ ,vocab] %*% t(gamma[ , vocab ])
dtm_topic  <-  as.matrix(dtm_topic)
e_dist  <-  CalcHellingerDist(dtm_topic)
e_measure  <-  (1 - e_dist)*100
diag(e_measure)  <-  0
Closest.Neighbors = 5
e_measure1 <- apply(e_measure, 1, function(x){
x[x < sort(x, decreasing = TRUE)[Closest.Neighbors]] <- 0
x
})
e_measure2 <- pmax(e_measure1, t(e_measure1))
e_measure3 <- graph.adjacency(e_measure2, mode = "undirected", weighted = TRUE)
ev <- evcent(e_measure3)
result <- sentences[names(ev$vector)[order(ev$vector,decreasing = TRUE)[1:Number.of.Sentences]]]
result <- result[order(as.numeric(names(result)))]
result <- paste(result, collapse = " ")
return(result)
}
corpus_text_vec <- sapply(corpus.txt, as.character)
# Test perplexity and find the best number of topics.
library(topicmodels)
dtm_list <- lapply(corpus.txt, function(doc) {
# Preprocess the document
# Create a DTM for the document
dtm <- DocumentTermMatrix(corpus.txt)
return(dtm)
})
# create two empty vector
best_topic_num= c()
perplexity_values=c()
# Set the number of topics
num_topics <- 2:8
for (i in 1:10){
dtm_matrix <- as.matrix(dtm_list[[i]])
# remove the row with all zero
dtm_matrix <- dtm_matrix[rowSums(dtm_matrix) > 0, ]
# Fit the LDA model
for (j in num_topics){
lda_model <- LDA(dtm_matrix, k = j)
# Calculate perplexity
perplexity <- perplexity(lda_model, newdata = dtm_matrix)
perplexity_values <- c(perplexity_values, perplexity)
}
# Find the index for the best perplexity values and record as topic number
best <- which.min(perplexity_values)+1
perplexity_values=c()
best_topic_num= c(best_topic_num,best)
}
best_topic_num
k=c(2,2,2,2,2,2,7,2,2,2)
# summary of the document
# For Number.of.Sentences usually choose 5%-20%, test the result and 15% is the best for my doc
# For Number.of.Topics, I calculate best_topic_num to get the best number of topic
for (i in 1:length(corpus.txt)){
summary = embedding(corpora=list(id = files.txt[i],content=corpus_text_vec[i]),Number.of.Sentences = k[i], Number.of.Topics = best_topic_num[i])
cat("Summary: \n", files.txt[i], ":\n\n", summary, '\n\n')
}
